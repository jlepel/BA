%\setcounter{secnumdepth}{3}
\chapter{Einleitung}
Aufgabe: Vom Problem zur Fragestellung

\begin{quote}
	``Es ist nicht zu wenig Zeit, die wir haben, sondern es ist zu viel Zeit, die wir nicht nutzen'' - Lucius Annaeus Seneca, \cite{Apelt200511}
\end{quote}
dieser Satz stammt von dem römischer Philosophen Seneca (ca. 49 n. Chr.) und besitzt noch heute in der Informatik seine Daseinsberechtigung.
Die Informatik hat durch Automatisierung Prozessabläufe vereinfacht, durch effizientere Ressourcennutzung Zeit eingespart und den Arbeitsalltag beschleunigt.
Gerade durch das Erforschen von effizienterer Ressourcennutzung und der Suche nach Zeitersparnis, ist in der Informatik die Virtualisierung entstanden. Sie hat in vielen Firmen die Serverlandschaften deutlich verkleinert, Clients durch virtuelle Desktops ersetzt und gestattet es Entwicklern und Testern ihren Arbeitsbereich auf produktionsnahen virtuellen Klonen auszuführen.
Gerade der letzt genannte Aspekt, ist die Grundlage für diese Arbeit. Um genauer zu sein, wird der Fokus auf den automatisierten Aufbau von Entwicklungsumgebungen gelegt, inklusive der Vervollständigung durch ausgewählte Softwarekomponenten. Es wird also versucht, dem Entwickler Zeit zu sparen, damit er seine Zeit effektiver nutzen kann. Gewünschter Nebeneffekt ist die Unabhängigkeit von IT-Support Instanzen. So wird der Anwender in die Lage versetzt autonom zu handeln und Bearbeitungsschritte zu beschleunigen.

\begin{comment}
http://www.golem.de/news/openstack-viele-brauchen-es-keiner-versteht-es-wir-erklaeren-es-1503-112814.html

Jeder Administrator kennt konventionelle IT-Einrichtungen, wie sie über Jahre hinweg beinahe überall zum Einsatz kamen. Klassische IT-Setups haben hinsichtlich ihrer Architektur eines gemeinsam: Es gibt Server mit festgelegten Funktionen, und die einmal bestimmten Funktionen verändern sich im Laufe des Lebens eines Servers auch nicht mehr.

Ein typisches Beispiel ist ein Monitoring-Server: Ein Stück Hardware ist dazu da, um einen einzigen Zweck zu erfüllen, und bekommt höchstens noch einen Zwilling als HA-Cluster. Datenbanken, Webserver, Applikationsserver, Archivserver - all diese Rollen sind typisch in alltäglichen Installationen. Fällt der Monitoring-Server aus, übernimmt im besten Falle dessen Cluster-Partner, und der Administrator hat die Möglichkeit, den ausgefallenen Rechner zu reparieren. Außerdem ist die gesamte Architektur einer Plattform auf diese spezifischen Eigenschaften der Systeme ausgerichtet: Auf den beteiligten Switches findet sich eine starre, im Switch abgelegte Zuordnung von VLANs. Neue Rechner kommen selten hinzu. Kurzum: Die ganze Infrastruktur ist in ihrer Anlage bereits sehr festgelegt.

Die Herausforderung klassischer IT-Umgebungen besteht daher oft darin: Wenn das Unternehmen wächst, das das Setup betreibt, muss auch die IT-Plattform mit ihm wachsen. Die konventionelle Methode macht das aber äußerst schwierig, denn neue Funktionen im Setup setzen neue Rechner voraus, und an der Installation neuer Rechner hängt eine ganze Reihe weiterer Abhängigkeiten wie die korrekte Einrichtung von Switches und die spezifische Installation der benötigten Softwarekomponenten.

All das erfordert Zeit und Arbeitskraft. Administratoren sind regelmäßig dieser eher eintönigen Arbeit ausgesetzt, statt sich um Innovation und höheren Komfort zu kümmern. Außerdem nutzen konventionelle Installationen ihre Ressourcen häufig schlecht: Ein moderner Server mit etlichen CPUs und viel Arbeitsspeicher ist mit einem einzelnen Webserverprozess kaum auszulasten, wenn das Setup nicht Java oder ähnlich rechenintensive Software nutzt.

irtualisierung von Computing-Leistungen war der erste Schritt auf dem Weg von spezifischen Rechnern zu universell einsetzbaren. Sie verfolgt dabei einen einfachen Zweck - die Entkoppelung von Hardware - und sorgt dafür, dass eine IT-Infrastruktur deutlich flexibler wird. Ein Server dient nicht mehr nur einer Aufgabe, sondern erfüllt verschiedene Funktionen - abhängig davon, wie viele virtuelle Maschinen (VMs) auf jenem System beheimatet sind.

Typische Virtualisierungen mittels KVM oder VMware lösen aber nur einen Teil der typischen Probleme. Denn auch bei ihnen ist der Grad an Automatisierung eher gering: Zwar müssen Administratoren nicht mehr für jedes neue System auch neue Server in Racks schrauben, aber neue virtuelle Maschinen legen sie noch immer per Hand an. Ebenfalls manuell werden Netzwerke eingerichtet und persistenter Speicher kommissioniert, denn wer will schon seinen Kunden etwa den Zugriff auf vorhandene, zentrale SAN-Speicher erlauben?

Nicht wenige Administratoren unterstellen auch deshalb, dass sich Virtualisierung bis vor rund zwei Jahren ausschließlich auf die Computing-Dienstleistung bezogen habe, während klassische Elemente eines Setups wie Speicher und Storage gar nicht virtualisiert gewesen seien. Das hat sich inzwischen durch eine Erweiterung der Virtualisierung deutlich geändert.

Jetzt verhält es sich in Cloud-Computing-Installationen so: Jeder Rechner ist eine Computing-Maschine und jede angebotene Dienstleistung ist virtualisiert. Das erstreckt sich sowohl auf den Computing-Teil als auch auf das Thema Storage und inzwischen auch auf die Netzwerkverwaltung. Anwender bedienen sich selbst, indem sie etwa nach Bedarf (on demand) neue, virtuelle Systeme selbst starten. Dazu stellt der Anbieter ihnen ein entsprechendes Portal zur Verfügung.

Damit das alles so funktioniert, müssen im Hintergrund etliche Programme arbeiten. Diese steuern die Virtualisierung des Computings, legen dynamische Netzwerke an, die ohnehin nur noch virtuell existieren und hängen an Kunden-VMs Speicher an, wenn sie benötigt werden. Braucht ein Kunde schnell neue Webserver, weil etwa sein Onlineshop im Fernsehen erwähnt worden ist, startet er die neuen virtuellen Maschinen einfach so, wie er sie gerade benötigt. Der Internetanbieter wird gewissermaßen degradiert: In einer typischen Cloud ist er nur noch derjenige, der die Infrastruktur zur Verfügung stellt.

In einer Cloud ist alles durch Software bestimmt (Software-defined), die entlang der vom Administrator festgelegten Richtlinien automatisch Dinge erledigt - willkommen bei Openstack!
\end{comment}

\begin{comment}
Virtualisierung hat die Schlagzahl beträchtlich erhöht ? mit KVM, Xen, VMware & Co. stellt der Admin neue Systeme viel schneller bereit, als sich neue Hardware in den Zeiten dedizierter Server beschaffen ließ. Die gewonnene Zeit bringt der nun für viel mehr Systeme Verantwortliche mit dem Klonen von Image-Konfigurationen, dem Kopieren von Verzeichnissen und dem Mounten von Speicherressourcen zu. Richtig knifflig wird es, wenn die Hardware nach Wartung ruft, denn dann gilt es, virtuelle Maschinen herunterzufahren, zu verschieben und neu zu starten.

Wer die Schwemme an Handgriffen reduzieren will, greift zu einer Infrastructure-as-a-Service-Lösung (IaaS). Sie nimmt sich der wichtigsten Aufgaben der Servervirtualisierung an, verwaltet Basisinfrastruktur wie DNS und DHCP und stellt dem Admin eine Weboberfläche zur Verfügung. Mit Open Stack [1], Open QRM [2], Eucalyptus [3] oder Ganeti [4] darf der Systemverwalter aus Open-Source-Produkten unterschiedlicher Herkunft und Funktionsumfangs und mit jeweils eigenen Konzepten wählen.
\end{comment}



\section{Motivation}
Das Ziel dieser Arbeit ist die Entwicklung einer Applikation, zur automatisierten Erstellung von Linux basierten ad hoc Umgebungen. Der Anwender soll in die Lage versetzt werden, ohne Grundlegende Kenntnisse über Anwendungsstruktur und Betriebssystem, mit minimalem Aufwand, die benötigte virtuelle Maschine zu erstellen.
Großer zeitlicher Aufwand für die Erstellung und Organisation wird entsprechend vermieden und es Anwendern ermöglicht sich auf Kerntätigkeiten zu fokussieren.
Somit muss keine Zeit mehr in Aufbau, Installation und Problembehebung investieren werden.

Der normalerweise große zeitliche Aufwand für die Erstellung von virtuellen Maschinen soll möglichst minimiert werden und es Anwendern sowohl in Unternehmen als auch bei  Projektarbeiten erleichtern, sich auf die vorhandenen Usecase zu fokussieren. Somit muss keine Zeit mehr in Aufbau, Installation und Problembehebung investieren werden.

die durch vereinfachte Handhabung und einer minimalen Einarbeitungszeit den Anwender in die Lage versetzt schnellst möglich eine ad hoc Entwicklungsumgebung zu erstellen. Dies soll 

Das Ziel dieser Arbeit ist es, eine Software zu entwickeln, die durch vereinfachte Handhabung und minimaler Einarbeitungszeit, es dem Benutzer ermöglich eine ad-hoc Umgebung zu erstellen, ohne dass ein bürokratischer Aufwand erforderlich ist und ohne Grundwissen über die darunterliegende Anwendungsstruktur.
Der normalerweise große zeitliche Aufwand für die Erstellung von virtuellen Maschinen soll möglichst minimiert werden und es Anwendern sowohl in Unternehmen als auch bei  Projektarbeiten erleichtern, sich auf die vorhandenen Usecase zu fokussieren. Somit muss keine Zeit mehr in Aufbau, Installation und Problembehebung investieren werden.

\section{Zielsetzung}
Das Ziel der vorliegenden Arbeit ist es, durch inkrementelles und interaktives Vorgehen eine Applikation zu modellieren, die den Anwender der Applikation beim Aufbau von virtuellen Umgebungen unterstützt. Je nach Wunsch des Anwenders, wird nicht nur der Aufbau einer Umgebung vereinfacht, sondern auch die direkte Installation von Programmen veranlasst. Eine Weboberfläche soll die entsprechenden Optionen zur Verfügung stellen und den Anwender durch seine ausgewählte Funktion leiten.
Große Konfigurationen oder komplizierte Einstellungen sollen dem Anwender abgenommen werde. Sie geschehen im Hintergrund. 
Damit auch ein Sichern oder ein Zurückspielen von vorhandenen virtuellen Maschinen möglich wird, sollen Im- und Exportfunktionen dies untzerstützen.
Die Realisierung soll auf einem zentralen Knotenpunkt stattfinden, um es mehreren Anwendern zu ermöglichen, ihre notwendigen Maschinen zu erstellen und zu verwalten.
Kernaufgaben sollen Open-Source Anwendungen übernehmen, die in ihrem Einsatzbereich etabliert sind. Ebenfalls im Fokus steht die Leichtigkeit der Konfiguration der auszuwählenden Open-Source Anwendung.
Bei der Erstellung der einzelnen Softwarekomponenten sind stehts die Prinzipien von hoher Kohäsion und loser Kopplung zu beachten. Darunter wird der Grad der Abhängigkeit zwischen mehreren Hard- und Softwarekomponenten verstanden, der Änderungen an einzelnen Komponenten erleichtert.
Um auch die Anwendungsoberfläche unkompliziert zu halten, soll der Anwender mit ein paar Klicks zu seinem Ziel geführt werden. Durch das Vermeiden sowohl von unnötigen verschachtelten Menüführungen, als auch von einer Vielfalt an Optionen und Konfigurationen, soll der Anwender in der Applikation einen Helfer für seine Tätigkeit finden.
\newline


\begin{comment}


\section{Problemstellung}
%Die Einleitung muss Ihr Thema eingrenzen (und diese Eingrenzung rechtfertigen) und Ihr Erkenntnisinteresse prÃ¤zisieren und begrÃ¼nden
Virtualisierung hat in vielen Bereichen den physischen Server abgelöst, denn der Finanzielle Aspekt ist für Unternehmen nicht unerheblich. Im Idealfall heißt der Umstieg auf virtuelle Landschaften gleich weniger Server, was gleichbedeutend mit weniger Stellfläche ist. Somit auch mit weniger Racks und weniger Verkabelung.\newline
Aufwändige Vorplanung von Serverzentren entfällt, die Kostenplanung der unterschiedlichen Hardware wird minimiert und die Frage, was in ein paar Jahren mit der Hardware passieren soll, wird obsolet.\newline
Gerade im Entwicklungsbereich ist es meist sinnvoller virtuelle Umgebungen zu realisieren, als reale Maschinen aufzubauen. Entwickler haben so die Möglichkeit bei Bedarf sich Abz"uge der Produktionsumgebung zu erstellen oder Fehlerszenarien nachzustellen.\newline
Meist ist dazu die Involvierung des Betriebs-Teams oder des IT-Support notwendig, die nach Priorität ihrer Auftragslage, eine gewissen Vorlaufzeit benötigen, um die gewünschte Maschine aufzubauen. \newline
In dem Fall, dass die Firmengröße es nicht erlaubt, eine eigene Support-Abteilung zu haben, muss die Zeit des jeweiligen Mitarbeiters herhalten, um das Wissen über die jeweilige Virtualisierungslösung aufzubauen, die gewünschte Maschine zu erstellen und die Installationen der nötigen Programme zu realisieren. Der Rückschluss daraus ist, geringere Produktivität in den Kerntätigkeiten des Mitarbeiters.\newline
Auch wenn die Softwarebranche eine Vielfalt an Möglichkeiten bereitstellt, sind diese entweder in ihrer Struktur überdimensioniert, um sie in der Anwendung schnell zu erlernen, oder komplex in ihrer Konfiguration in Bezug auf Automatisierungen und/oder Provisionierungen.

\end{comment}

\section{Themenabgrenzung}
Diese Arbeit greift bekannte und etablierte Softwareprodukte auf und nutzt diese in einem zusammenhängenden Kontext. Dabei werden die verwendeten Softwareprodukte nicht modifiziert, sondern für eine vereinfachte Benutzung durch eigene Implementierungen kombiniert. Sie werden mit einem Benutzerinterface versehen, welches die Abläufe visualisiert und dem Benutzer die Handhabung vereinfacht.
Die vorzunehmenden Implementierungen greifen nicht in den Ablauf der jeweiligen Software ein, sondern vereinfachen das Zusammenspiel der einzelnen Anwendungen.



\section{Struktur der Arbeit}
Diese Arbeit wird in Kapitel \ref{ch:Grundlagen} Grundlagen schaffen, um ein besseres Verständnis über die Virtualisierung zu erhalten 

